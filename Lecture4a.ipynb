{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAMIDSpiyalong/AdvancedML/blob/main/Lecture4a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t81VT9JboTzt"
      },
      "source": [
        "# Basic Bag-of-Words (BOW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_EAof8njfHz"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from scipy import spatial\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1IVdG29wyJ7"
      },
      "source": [
        "Plain frequency BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fwfWQDVyJpY"
      },
      "outputs": [],
      "source": [
        "# A corpus of sentences.\n",
        "corpus = [\n",
        "  \"Red Bull drops hint on F1 engine.\",\n",
        "  \"Honda exits F1, leaving F1 partner Red Bull.\",\n",
        "  \"Hamilton eyes record eighth F1 title.\",\n",
        "  \"Aston Martin announces sponsor.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILvS020Zzm6F"
      },
      "source": [
        "We want to build a basic bag-of-words (BOW) representation of our corpus. You can probably do this from scratch using dictionaries and lists (and maybe that's a good exercise). Fortunately, there are robust libraries which make it easy. We can use the scikit-learn **CountVectorizer** which takes a collection of text documents and creates a matrix of token counts:<br>\n",
        "https://scikit-learn.org/stable/index.html<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRhJPxbHwuj_"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAphZMVPBX9P"
      },
      "source": [
        "The *fit_transform* method does two things:\n",
        "1. It learns a vocabulary dictionary from the corpus.\n",
        "2. It returns a matrix where each row represents a document and each column represents a token (i.e. term).<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5wi4_C7BAWv"
      },
      "outputs": [],
      "source": [
        "bow = vectorizer.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Bp1XNcF1FQ"
      },
      "source": [
        "We can take a look at the features and vocabulary dictionary. Notice the **CountVectorizer** took care of tokenization for us. It also removed punctuation and lower-cased everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlTKj8GPbQpD"
      },
      "outputs": [],
      "source": [
        "vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bywJ0XnGKPQ"
      },
      "source": [
        "If we look at the raw structure, we'll see tuples where the first element represents the document, and the second element represents a token ID. It's then followed by a count of that token. So in the second document (index 1), token 8 (\"f1\") occurs twice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At6Gt4bsEx2D"
      },
      "outputs": [],
      "source": [
        "for i in bow:\n",
        "    print(i.toarray()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF0NVhdEUR1r"
      },
      "source": [
        "## Cosine Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leI1VuDVVP4W"
      },
      "source": [
        "One way is using the **spatial** package, which is a collection of spatial algorithms and data structures. It has a method to calculate cosine *distance*. To get the cosine *similarity*, we have to substract the distance from 1.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOQQ50IgXQfH"
      },
      "outputs": [],
      "source": [
        "# The cosine method expects array_like inputs, so we need to generate\n",
        "# arrays from our sparse matrix.\n",
        "doc1_vs_doc2 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[1].toarray()[0])\n",
        "doc1_vs_doc3 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[2].toarray()[0])\n",
        "doc1_vs_doc4 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[3].toarray()[0])\n",
        "\n",
        "print(corpus)\n",
        "\n",
        "print(f\"Doc 1 vs Doc 2: {doc1_vs_doc2}\")\n",
        "print(f\"Doc 1 vs Doc 3: {doc1_vs_doc3}\")\n",
        "print(f\"Doc 1 vs Doc 4: {doc1_vs_doc4}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnC_i4oH2ARW"
      },
      "source": [
        "# TF-IDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMwv39AfP7Ti"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmcTBtSx-XqZ"
      },
      "source": [
        "## Fetching datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYkq3i7_-qhQ"
      },
      "source": [
        "This time around, rather than using a short toy corpus, let's use a larger dataset. scikit-learn has a **datasets** module with utilties to load datasets of our own as well as fetch popular reference datasets online.<br>\n",
        "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYjxqxVBBINV"
      },
      "source": [
        "The **datasets** module includes fetchers for each dataset in scikit-learn. For our purposes, we'll fetch only the posts from the *sci.space* topic, and skip on headers, footers, and quoting of other posts.\n",
        "By default, the fetcher retrieves the *training* subset of the data only. If you don't know what that means, it'll become clear later in the course when we discuss modelling. For now, it doesn't matter for our purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9to6gQNCGiN"
      },
      "outputs": [],
      "source": [
        "corpus = fetch_20newsgroups(categories=['sci.space'],\n",
        "                            remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W989GHQxDvTW"
      },
      "source": [
        "We get back a **Bunch** container object containing the data as well as other information.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html\n",
        "<br><br>\n",
        "The actual posts are accessed through the *data* attribute and is a list of strings, each one representing a post."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POGdVmdIDuCK"
      },
      "outputs": [],
      "source": [
        "print(type(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6AgmbL0ES9I"
      },
      "outputs": [],
      "source": [
        "# Number of posts in our dataset.\n",
        "len(corpus.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAjM4uNDEXGf"
      },
      "outputs": [],
      "source": [
        "# View first two posts.\n",
        "print(corpus.data[:2][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH99M6cxCpsz"
      },
      "source": [
        "## Creating TF-IDF features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtnQX-wWDhGh"
      },
      "outputs": [],
      "source": [
        "# Like before, if we want to use spaCy's tokenizer, we need\n",
        "# to create a callback. Remember to upgrade spaCy if you need\n",
        "# to (refer to beginnning of file for commentary and instructions).\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# We don't need named-entity recognition nor dependency parsing for\n",
        "# this so these components are disabled. This will speed up the\n",
        "# pipeline. We do need part-of-speech tagging however.\n",
        "unwanted_pipes = [\"ner\", \"parser\"]\n",
        "\n",
        "# For this exercise, we'll remove punctuation and spaces (which\n",
        "# includes newlines), filter for tokens consisting of alphabetic\n",
        "# characters, and return the lemma (which require POS tagging).\n",
        "def spacy_tokenizer(doc):\n",
        "  with nlp.disable_pipes(*unwanted_pipes):\n",
        "    return [t.lemma_ for t in nlp(doc) if \\\n",
        "            not t.is_punct and \\\n",
        "            not t.is_space and \\\n",
        "            t.is_alpha]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il-0gY9LEiNv"
      },
      "source": [
        "Like the classes to create raw frequency and binary bag-of-words vectors, scikit-learn includes a similar class called **TfidfVectorizer** to create TF-IDF vectors from a corpus.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "<br><br>\n",
        "The usage pattern is similar in that we call *fit_transform* on the corpus which generates the vocabulary dictionary (fit step), and generates the TF-IDF vectors (transform step)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Shj6BS0BN6FU"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Use the default settings of TfidfVectorizer.\n",
        "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
        "features = vectorizer.fit_transform(corpus.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ9w4gh9sobB"
      },
      "outputs": [],
      "source": [
        "# The number of unique tokens.\n",
        "print(len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CxmKlPcNRLk"
      },
      "outputs": [],
      "source": [
        "# The dimensions of our feature matrix. X rows (documents) by Y columns (tokens).\n",
        "print(features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJwnU8PZNdHU"
      },
      "outputs": [],
      "source": [
        "# What the encoding of the first document looks like in sparse format.\n",
        "print(features[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp7VTwYzONlt"
      },
      "source": [
        "As we mentioned in the slides, there are TF-IDF variations out there and scikit-learn, among other things, adds **smoothing** (adds a one to the numerator and denominator in the IDF component), and normalizes by default. These can be disabled if desired using the *smooth_idf* and *norm* parameters respectively. See here for more information:<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylKLM-IMOwbJ"
      },
      "source": [
        "## Querying the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8oTtCg0QB71"
      },
      "source": [
        "The similarity measuring techniques we learned previously can be used here in the same way. In effect, we can query our data using this sequence:\n",
        "1. *Transform* our query using the same vocabulary from our *fit* step on our corpus.\n",
        "2. Calculate the pairwise cosine similarities between each document in our corpus and our query.\n",
        "3. Sort them in descending order by score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNjEUzqlP6Oy"
      },
      "outputs": [],
      "source": [
        "# Transform the query into a TF-IDF vector.\n",
        "query = [\"lunar orbit\"]\n",
        "query_tfidf = vectorizer.transform(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEfdfkmpP8Tv"
      },
      "outputs": [],
      "source": [
        "# Calculate the cosine similarities between the query and each document.\n",
        "# We're calling flatten() here becaue cosine_similarity returns a list\n",
        "# of lists and we just want a single list.\n",
        "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skuSFhLxXOMC"
      },
      "source": [
        "Now that we have our list of cosine similarities, we can use this utility function to return the indices of the top k documents with the highest cosine similarities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0PvqRDpUSYO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# numpy's argsort() method returns a list of *indices* that\n",
        "# would sort an array:\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
        "#\n",
        "# The sort is ascending, but we want the largest k cosine_similarites\n",
        "# at the bottom of the sort. So we negate k, and get the last k\n",
        "# entries of the indices list in reverse order. There are faster\n",
        "# ways to do this using things like argpartition but this is\n",
        "# more succinct.\n",
        "def top_k(arr, k):\n",
        "  kth_largest = (k + 1) * -1\n",
        "  return np.argsort(arr)[:kth_largest:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFYpEldVUaAG"
      },
      "outputs": [],
      "source": [
        "# So for our query above, these are the top five documents.\n",
        "top_related_indices = top_k(cosine_similarities, 5)\n",
        "print(top_related_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e86P3bQR1ZS"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at their respective cosine similarities.\n",
        "print(cosine_similarities[top_related_indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzdyTptURiTQ"
      },
      "outputs": [],
      "source": [
        "# Top match.\n",
        "print(corpus.data[top_related_indices[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQwWXypfR8vh"
      },
      "outputs": [],
      "source": [
        "# Second-best match.\n",
        "print(corpus.data[top_related_indices[1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-5aqUbGSM5J"
      },
      "outputs": [],
      "source": [
        "# Try a different query\n",
        "query = [\"satellite\"]\n",
        "query_tfidf = vectorizer.transform(query)\n",
        "\n",
        "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()\n",
        "top_related_indices = top_k(cosine_similarities, 5)\n",
        "\n",
        "print(top_related_indices)\n",
        "print(cosine_similarities[top_related_indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHQtRQIcSbTj"
      },
      "outputs": [],
      "source": [
        "print(corpus.data[top_related_indices[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4v5wQ4JaBIh"
      },
      "source": [
        "So here we have the beginnings of a simple search engine but we're a far cry from competing with commercial off-the-shelf search engines, let alone Google.\n",
        "<br>\n",
        "- For each query, we're scanning through our entire corpus, but in practice, you'll want to create an **inverted index**. Search applications such as Elasticsearch do that under the hood.\n",
        "- You'd also want to evaluate the efficacy of your search using metrics like **precision** and **recall**.\n",
        "- Document ranking also tends to be more sophisticated, using different ranking functions like Okapi BM25. With major search engines, ranking also involves hundreds of variables such as what the user searched for previously, what do they tend to click on, where are they physically, and on and on. These variables are part of the \"secret sauce\" and are closely guarded by companies.\n",
        "- Beyond word presence, intent and meaning are playing a larger role.\n",
        "<br>\n",
        "\n",
        "Information Retrieval is a huge, rich topic and beyond search, it's also key in tasks such as question-answering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak3LXiETfGIY"
      },
      "source": [
        "# Using Pretrained, Third-Party Vectors\n",
        "There are a variety of pretrained, static word vector packages out there. In this section, we'll use the **Google News** vectors, a collection of three million, 300-dimension word vectors trained from three billion words from a Google News corpus (circa 2015)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58V-1VqHbQpI"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import spacy\n",
        "import tensorflow as tf\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz2FCCq1fsjz"
      },
      "outputs": [],
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1BpfbHu4denceXiv8yfdY3EHgjKIcULku\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9roZ_SeybQpJ"
      },
      "outputs": [],
      "source": [
        "embedding_file = 'GoogleNews-vectors-negative300.bin.gz'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQkCrzSEbQpJ"
      },
      "source": [
        "Next, we'll have **gensim** load the vectors through the **KeyedVectors** module which will enable us to look up vectors by tokens and indices.<br>\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html\n",
        "<br><br>\n",
        "To save time and space, we'll limit ourselves to 200,000 word vectors for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVeb02mqbQpJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jpuGamMbQpJ"
      },
      "outputs": [],
      "source": [
        "word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True, limit=200000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11hDkconbQpJ"
      },
      "outputs": [],
      "source": [
        "len(word_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh1WY8agbQpJ"
      },
      "outputs": [],
      "source": [
        "pizza = word_vectors['pizza']\n",
        "print(f'Vector dimension: {pizza.shape}')\n",
        "\n",
        "# The embedding for the word 'pizza'.\n",
        "print(pizza)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFArLjMJbQpJ"
      },
      "outputs": [],
      "source": [
        "print(word_vectors.similarity('pizza', 'gorilla'))\n",
        "print(word_vectors.similarity('pizza', 'tree'))\n",
        "print(word_vectors.similarity('pizza', 'yoga'))\n",
        "print(word_vectors.similarity('pizza', 'tomato'))\n",
        "print(word_vectors.similarity('pizza', 'sauce'))\n",
        "print(word_vectors.similarity('pizza', 'cheese'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR2dE67bbQpK"
      },
      "outputs": [],
      "source": [
        "word_vectors.n_similarity(\"dog bites man\".split(), \"canine nips human\".split())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf6NYTCcbQpK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxnPBIwDbQpK"
      },
      "source": [
        "Visualizing word vectors is straight-forward and can offer insights into what kind of contexts the training algorithm picked up. Because these word vectors have a dimension of 300, we need to reduce them down to two dimensions to plot them on a regular graph. This can be done through **Principal Components Analysis (PCA)**. Here, we're plotting the words we considered in the slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80rVbPWdbQpK"
      },
      "outputs": [],
      "source": [
        "def display_pca_scatterplot(model, words):\n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r', s=128)\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)\n",
        "\n",
        "display_pca_scatterplot(word_vectors, ['swim', 'swimming', 'cat', 'dog', 'feline', 'road', 'car', 'bus'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqfpDplgbQpK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}